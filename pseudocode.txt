

Solution 1. LULLLULLUUUULLU

labeled = np.random.binomial(1, 0.3, 10) > 0 #[True, True, False, True, False]

for t in labeled:
    if t:
        train_targets[i] = None

for epoch in epochs:
    for batch in batches:

        #batch = [LUULULLLL]

        loss = 0
        for sample in batch:
            x, target = sample

            if target is not None: 
                loss += class_loss(x)
            
            loss += consistency_loss(x)

        loss.backward()
        optimizer.step()
        ema_optimizer.step() #theta <-

        #end of the current batch
        
    #end of the current epoch